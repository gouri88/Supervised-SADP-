{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f330ad-3e2d-4ae2-b21a-7c5f2aef415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import layers, models\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d174e32c-79cb-457b-8592-33a56f32d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "seed = 42\n",
    "encoder_epochs = 100\n",
    "batch_size_encoder = 128\n",
    "pretrain_encoder = True   # set False to skip\n",
    "feature_dim = 256         # encoder output size\n",
    "Nhid = 300                # hidden spiking neurons\n",
    "Nout = 10                 # CIFAR-10 classes\n",
    "T = 25                    # timesteps for spikes\n",
    "\n",
    "# Spiking neuron parameters\n",
    "lam = 0.9              # leak (membrane decay)\n",
    "theta_h_base = 0.5     # hidden threshold base\n",
    "theta_o = 0.5          # output threshold\n",
    "eta_out = 5e-4         # LR for W2 (supervised Hebbian)\n",
    "eta_in = 2e-4          # LR for W1 (κ-based SADP)\n",
    "decay = 0.9995         # mild decay (multiplicative) per update\n",
    "norm_eps = 1e-6\n",
    "clip_w2 = 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03fbdb1-697f-4a64-9de5-7c56763e95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name='mnist'):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses MNIST, Fashion-MNIST, or CIFAR-10.\n",
    "    Returns (x_train, y_train), (x_test, y_test), and input_shape.\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    if name == 'mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    elif name in ['fmnist', 'fashion_mnist']:\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    elif name == 'cifar10':\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be one of: 'mnist', 'fmnist', or 'cifar10'.\")\n",
    "\n",
    "    # Normalize\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test  = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    # Ensure channel dimension\n",
    "    if x_train.ndim == 3:   # grayscale\n",
    "        x_train = np.expand_dims(x_train, -1)\n",
    "        x_test  = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(f\"Loaded {name.upper()} dataset with shape: {input_shape}\")\n",
    "    print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4175044c-1d08-4ffb-85cc-cc34a62c216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Poisson Encoder\n",
    "# ---------------------------\n",
    "def poisson_encode_features(batch_feats, T):\n",
    "    B, Nin_local = batch_feats.shape\n",
    "    rnd = np.random.rand(B, T, Nin_local).astype(np.float32)\n",
    "    spikes = (rnd < batch_feats[:, None, :]).astype(np.float32)\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa84d2b-43cc-4c15-a0d7-dc0dfaa7cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(input_shape=(32,32,3), output_dim=feature_dim):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inp)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(output_dim, activation='sigmoid')(x)\n",
    "    return models.Model(inp, x, name='encoder_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2b2cc4-4570-4b47-b6f0-45a6a480b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LIF forward pass (1SADP / 2SADP)\n",
    "def forward_lif_features(x_batch_feats, y_targets=None, teacher_force=False,\n",
    "                         architecture='1SADP', T_local=25):\n",
    "    global W1, W1_2, W2, theta_h, theta_h2, Nin, Nout, Nhid_global, lam, theta_o\n",
    "\n",
    "    B = x_batch_feats.shape[0]\n",
    "\n",
    "    # --------------------------\n",
    "    # Membrane potentials\n",
    "    # --------------------------\n",
    "    Vh  = np.zeros((B, Nhid_global), dtype=np.float32)\n",
    "    Vh2 = np.zeros((B, Nhid_global), dtype=np.float32) if architecture == '2SADP' else None\n",
    "    Vo  = np.zeros((B, Nout), dtype=np.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Spike storage\n",
    "    # --------------------------\n",
    "    spikes_h  = np.zeros((B, T_local, Nhid_global), dtype=np.float32)\n",
    "    spikes_h2 = np.zeros((B, T_local, Nhid_global), dtype=np.float32) if architecture=='2SADP' else None\n",
    "    spikes_o  = np.zeros((B, T_local, Nout), dtype=np.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Input Poisson spikes\n",
    "    # --------------------------\n",
    "    S_in = poisson_encode_features(x_batch_feats, T_local)\n",
    "\n",
    "    # --------------------------\n",
    "    # Time loop\n",
    "    # --------------------------\n",
    "    for t in range(T_local):\n",
    "\n",
    "        # ===== Hidden Layer 1 =====\n",
    "        I_h = np.einsum('bi,ij->bj', S_in[:, t], W1)\n",
    "        Vh = lam * Vh + I_h\n",
    "\n",
    "        spk_h = (Vh > theta_h).astype(np.float32)\n",
    "        Vh[spk_h == 1] = 0.0\n",
    "        spikes_h[:, t, :] = spk_h\n",
    "\n",
    "        # ===== Hidden Layer 2 (2SADP) =====\n",
    "        if architecture == '2SADP':\n",
    "            I_h2 = np.einsum('bi,ij->bj', spk_h, W1_2)\n",
    "            Vh2 = lam * Vh2 + I_h2\n",
    "\n",
    "            spk_h2 = (Vh2 > theta_h2).astype(np.float32)\n",
    "            Vh2[spk_h2 == 1] = 0.0\n",
    "            spikes_h2[:, t, :] = spk_h2\n",
    "\n",
    "            spk_to_output = spk_h2\n",
    "        else:\n",
    "            spk_to_output = spk_h\n",
    "\n",
    "        # ===== Output Layer =====\n",
    "        I_o = np.einsum('bi,ij->bj', spk_to_output, W2)\n",
    "        Vo = lam * Vo + I_o\n",
    "\n",
    "        spk_o = (Vo > theta_o).astype(np.float32)\n",
    "        Vo[spk_o == 1] = 0.0\n",
    "        spikes_o[:, t, :] = spk_o\n",
    "\n",
    "    # Return spikes\n",
    "    return (spikes_h, spikes_h2, spikes_o) if architecture=='2SADP' else (spikes_h, spikes_o)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Weight update per batch\n",
    "# ---------------------------\n",
    "# def update_weights_batch(x_batch_feats, y_batch, spikes_h, spikes_o, architecture='1SADP'):\n",
    "#     global W1, W1_2, W2\n",
    "\n",
    "#     B = x_batch_feats.shape[0]\n",
    "#     targets = np.zeros((B, Nout), dtype=np.float32)\n",
    "#     targets[np.arange(B), y_batch] = 1.0\n",
    "\n",
    "#     # output spike counts\n",
    "#     out_counts = spikes_o.sum(axis=1)\n",
    "#     preds = np.argmax(out_counts, axis=1)\n",
    "\n",
    "#     # Errors\n",
    "#     errors = targets[:, None, :] - spikes_o\n",
    "#     dW2 = np.einsum('bti,btj->ij', spikes_h if architecture=='1SADP' else spikes_h, errors) / float(B)\n",
    "#     W2 += eta_out * dW2\n",
    "\n",
    "#     # Kappa-based SADP for W1\n",
    "#     batch_idx = np.arange(B)[:, None]\n",
    "#     time_idx = np.arange(spikes_o.shape[1])[None, :]\n",
    "#     class_idx = y_batch[:, None]\n",
    "#     target_spikes = spikes_o[batch_idx, time_idx, class_idx].astype(np.float32)\n",
    "\n",
    "#     agree = np.mean(spikes_h == target_spikes[..., None], axis=1).astype(np.float32)\n",
    "#     pa = np.mean(spikes_h, axis=1).astype(np.float32)\n",
    "#     pb = np.mean(target_spikes, axis=1).astype(np.float32)[:, None]\n",
    "#     pe = pa * pb + (1 - pa) * (1 - pb)\n",
    "#     kappa_vals = (agree - pe) / (1.0 - pe + 1e-9)\n",
    "\n",
    "#     dW1 = np.einsum('bi,bj->ij', x_batch_feats, kappa_vals) / float(B)\n",
    "#     W1 += eta_in * dW1\n",
    "\n",
    "#     # decay & normalize\n",
    "#     W1 *= decay\n",
    "#     W2 *= decay\n",
    "#     W1 /= (np.linalg.norm(W1, axis=0, keepdims=True) + norm_eps)\n",
    "#     np.clip(W2, -clip_w2, clip_w2, out=W2)\n",
    "\n",
    "#     return preds, float(np.mean(kappa_vals))\n",
    "\n",
    "\n",
    "\n",
    "def update_weights_batch(x_batch_feats, y_batch,\n",
    "                         spikes_h, spikes_h2, spikes_o,\n",
    "                         architecture='1SADP'):\n",
    "    global W1, W1_2, W2\n",
    "\n",
    "    B, T, _ = spikes_o.shape\n",
    "\n",
    "    # ---------------------------\n",
    "    # One-hot targets\n",
    "    # ---------------------------\n",
    "    targets = np.zeros((B, Nout), dtype=np.float32)\n",
    "    targets[np.arange(B), y_batch] = 1.0\n",
    "\n",
    "    # ---------------------------\n",
    "    # Prediction\n",
    "    # ---------------------------\n",
    "    out_counts = spikes_o.sum(axis=1)\n",
    "    preds = np.argmax(out_counts, axis=1)\n",
    "\n",
    "    # ======================================================\n",
    "    # OUTPUT LAYER UPDATE (supervised Hebbian)\n",
    "    # ======================================================\n",
    "    errors = targets[:, None, :] - spikes_o\n",
    "    pre_out = spikes_h2 if architecture == '2SADP' else spikes_h\n",
    "\n",
    "    dW2 = np.einsum('bti,btj->ij', pre_out, errors) / B\n",
    "    W2 += eta_out * dW2\n",
    "\n",
    "    # ======================================================\n",
    "    # TARGET SPIKES (correct-class output neuron) \n",
    "    # ======================================================\n",
    "    target_spikes = spikes_o[np.arange(B), :, y_batch].astype(np.float32)\n",
    "\n",
    "    # ======================================================\n",
    "    # W1 SADP UPDATE (Input → Hidden-1)\n",
    "    # ======================================================\n",
    "    agree1 = np.mean(spikes_h == target_spikes[:, :, None], axis=1)\n",
    "\n",
    "    pa1 = np.mean(spikes_h, axis=1)\n",
    "    pb  = np.mean(target_spikes, axis=1)[:, None]\n",
    "\n",
    "    pe1 = pa1 * pb + (1.0 - pa1) * (1.0 - pb)\n",
    "    kappa1 = (agree1 - pe1) / (1.0 - pe1 + 1e-9)\n",
    "\n",
    "    dW1 = np.einsum('bi,bj->ij', x_batch_feats, kappa1) / B\n",
    "    W1 += eta_in * dW1\n",
    "\n",
    "    # ======================================================\n",
    "    # W1_2 SADP UPDATE (Hidden-1 → Hidden-2)\n",
    "    # ======================================================\n",
    "    if architecture == '2SADP':\n",
    "        agree2 = np.mean(spikes_h2 == target_spikes[:, :, None], axis=1)\n",
    "        pa2 = np.mean(spikes_h2, axis=1)\n",
    "\n",
    "        pe2 = pa2 * pb + (1.0 - pa2) * (1.0 - pb)\n",
    "        kappa2 = (agree2 - pe2) / (1.0 - pe2 + 1e-9)\n",
    "\n",
    "        dW1_2 = np.einsum(\n",
    "            'bi,bj->ij',\n",
    "            np.mean(spikes_h, axis=1),\n",
    "            kappa2\n",
    "        ) / B\n",
    "\n",
    "        W1_2 += eta_in * dW1_2\n",
    "        W1_2 *= decay\n",
    "        W1_2 /= (np.linalg.norm(W1_2, axis=0, keepdims=True) + norm_eps)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Decay & normalize\n",
    "    # ---------------------------\n",
    "    W1 *= decay\n",
    "    W2 *= decay\n",
    "\n",
    "    W1 /= (np.linalg.norm(W1, axis=0, keepdims=True) + norm_eps)\n",
    "    np.clip(W2, -clip_w2, clip_w2, out=W2)\n",
    "\n",
    "    return preds, float(np.mean(kappa1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a50730-11ab-489a-9553-d8b9b650e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(n_epochs=25, batch_size=128, n_samples=None):\n",
    "    global W1, W2   # ensure weight updates are applied\n",
    "    \n",
    "    n_train = len(train_feats_norm) if n_samples is None else min(n_samples, len(train_feats_norm))\n",
    "    n_epochs_run = 0\n",
    "    epoch_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        n_epochs_run += 1\n",
    "        t0 = time.time()\n",
    "\n",
    "        idx = np.random.permutation(len(train_feats_norm))[:n_train]\n",
    "        num_batches = n_train // batch_size\n",
    "\n",
    "        correct = 0\n",
    "        kappa_log = []\n",
    "\n",
    "        for bi in trange(num_batches, desc=f\"SNN Epoch {epoch+1}/{n_epochs}\"):\n",
    "            s = bi * batch_size\n",
    "            e = s + batch_size\n",
    "            batch_idx = idx[s:e]\n",
    "\n",
    "            Xb = train_feats_norm[batch_idx]\n",
    "            yb = y_train[batch_idx]\n",
    "\n",
    "            # forward\n",
    "            sh, so = forward_lif_features(Xb, teacher_force=False)\n",
    "\n",
    "           \n",
    "            preds, kappa_val = update_weights_batch(\n",
    "                Xb, yb,\n",
    "                spikes_h=sh,\n",
    "                spikes_h2=None,\n",
    "                spikes_o=so,\n",
    "                architecture='1SADP'\n",
    "            )\n",
    "\n",
    "            correct += np.sum(preds == yb)\n",
    "            kappa_log.append(kappa_val)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        acc = correct / float(n_train)\n",
    "        avg_k = float(np.mean(kappa_log)) if kappa_log else 0.0\n",
    "        epoch_accuracies.append(acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{n_epochs} | \"\n",
    "            f\"Train acc = {acc:.4f} | \"\n",
    "            f\"avg κ = {avg_k:.6f} | \"\n",
    "            f\"Time = {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    return None, n_epochs_run, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce669800-8d89-4d44-b9e5-5f40ed7ecad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Evaluate SNN Function\n",
    "# -----------------------------------------------------\n",
    "def evaluate_snn(n_samples=None, batch_size=256):\n",
    "    n_test = len(test_feats_norm) if n_samples is None else min(n_samples, len(test_feats_norm))\n",
    "    idx = np.random.permutation(len(test_feats_norm))[:n_test]\n",
    "    num_batches = int(np.ceil(n_test / batch_size))\n",
    "    correct = 0\n",
    "\n",
    "    for bi in trange(num_batches, desc=\"SNN Testing\"):\n",
    "        s = bi * batch_size\n",
    "        e = min(s + batch_size, n_test)\n",
    "        batch_idx = idx[s:e]\n",
    "\n",
    "        Xb = test_feats_norm[batch_idx]\n",
    "        yb = y_test[batch_idx]\n",
    "\n",
    "        sh, so = forward_lif_features(Xb, teacher_force=False)\n",
    "        preds = np.argmax(so.sum(axis=1), axis=1)\n",
    "        correct += np.sum(preds == yb)\n",
    "\n",
    "    acc = correct / float(n_test)\n",
    "    print(f\"SNN Test Acc = {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1070495-96b6-44cb-b022-0d05c02eb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Run Experiment\n",
    "# ---------------------------\n",
    "def run_experiment(dataset_name, feature_dim=128, encoder_epochs=10,\n",
    "                   batch_size_encoder=128, pretrain_encoder=True,\n",
    "                   Nhid=256, theta_h_base=0.5, seed=42,\n",
    "                   architecture='1SADP', T=25,\n",
    "                   encoding_type='cnn+poisson'):\n",
    "    \"\"\"\n",
    "    encoding_type: 'poisson_only' or 'cnn+poisson'\n",
    "    architecture: '1SADP' or '2SADP'\n",
    "    T: temporal dimension (timesteps)\n",
    "    \"\"\"\n",
    "\n",
    "    global train_feats_norm, test_feats_norm, y_train, y_test\n",
    "    global W1, W1_2, W2, theta_h, theta_h2, Nin, Nout, Nhid_global, lam, theta_o\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Running experiment on {dataset_name.upper()} | {architecture} | T={T} | Encoding: {encoding_type}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Load Dataset\n",
    "    # ---------------------------\n",
    "    (x_train, y_train), (x_test, y_test), input_shape = load_dataset(dataset_name)\n",
    "    if encoding_type == 'cnn+poisson':\n",
    "        encoder = build_encoder(input_shape=input_shape, output_dim=feature_dim)\n",
    "    \n",
    "        if pretrain_encoder:\n",
    "            inp = encoder.input\n",
    "            feat = encoder.output\n",
    "            out = layers.Dense(Nout, activation='softmax')(feat)\n",
    "            clf = models.Model(inp, out)\n",
    "            clf.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "    \n",
    "            print(f\"Pretraining encoder on {dataset_name.upper()}...\")\n",
    "            clf.fit(\n",
    "                x_train, y_train,\n",
    "                validation_split=0.1,\n",
    "                epochs=encoder_epochs,\n",
    "                batch_size=batch_size_encoder,\n",
    "                verbose=2\n",
    "            )\n",
    "\n",
    "\n",
    "        # ---------------------------\n",
    "        # Feature Extraction\n",
    "        # ---------------------------\n",
    "        print(\"Extracting features from CNN...\")\n",
    "        train_feats = encoder.predict(x_train, batch_size=256, verbose=1)\n",
    "        test_feats  = encoder.predict(x_test, batch_size=256, verbose=1)\n",
    "\n",
    "        # Normalize features\n",
    "        minf = train_feats.min(axis=0, keepdims=True)\n",
    "        maxf = train_feats.max(axis=0, keepdims=True)\n",
    "        rangef = (maxf - minf) + 1e-9\n",
    "        train_feats_norm = (train_feats - minf) / rangef\n",
    "        test_feats_norm  = (test_feats  - minf) / rangef\n",
    "\n",
    "    elif encoding_type == 'poisson_only':\n",
    "        # Flatten raw images and normalize to [0,1]\n",
    "        train_feats_norm = x_train.reshape(len(x_train), -1)\n",
    "        train_feats_norm = train_feats_norm.astype(np.float32)\n",
    "        train_feats_norm /= train_feats_norm.max()\n",
    "\n",
    "        test_feats_norm = x_test.reshape(len(x_test), -1)\n",
    "        test_feats_norm = test_feats_norm.astype(np.float32)\n",
    "        test_feats_norm /= test_feats_norm.max()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"encoding_type must be 'poisson_only' or 'cnn+poisson'\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Network parameters\n",
    "    # ---------------------------\n",
    "    Nin = train_feats_norm.shape[1]\n",
    "    Nout = len(np.unique(y_train))\n",
    "    Nhid_global = Nhid\n",
    "    lam = 0.9\n",
    "    theta_o = 0.5\n",
    "\n",
    "    # ---------------------------\n",
    "    # Initialize weights\n",
    "    # ---------------------------\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.normal(0, 0.1, (Nin, Nhid_global)).astype(np.float32)\n",
    "    W2 = np.random.normal(0, 0.1, (Nhid_global, Nout)).astype(np.float32)\n",
    "    theta_h = (theta_h_base + 0.05 * np.random.randn(Nhid_global)).astype(np.float32)\n",
    "\n",
    "    if architecture == '2SADP':\n",
    "        W1_2 = np.random.normal(0, 0.1, (Nhid_global, Nhid_global)).astype(np.float32)\n",
    "        theta_h2 = (theta_h_base + 0.05 * np.random.randn(Nhid_global)).astype(np.float32)\n",
    "        print(f\"Initialized second hidden layer W1_2: {W1_2.shape}, theta_h2: {theta_h2.shape}\")\n",
    "    else:\n",
    "        W1_2 = None\n",
    "        theta_h2 = None\n",
    "\n",
    "    print(f\"Feature dim (Nin): {Nin}, Nhid: {Nhid_global}, Nout: {Nout}\")\n",
    "    if architecture == '1SADP':\n",
    "        print(f\"Initialized W1: {W1.shape}, W2: {W2.shape}, theta_h: {theta_h.shape}\")\n",
    "    else:\n",
    "        print(f\"Initialized W1: {W1.shape}, W1_2: {W1_2.shape}, W2: {W2.shape}, theta_h: {theta_h.shape}, theta_h2: {theta_h2.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
