{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f330ad-3e2d-4ae2-b21a-7c5f2aef415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d174e32c-79cb-457b-8592-33a56f32d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "seed = 42\n",
    "encoder_epochs = 100\n",
    "batch_size_encoder = 128\n",
    "pretrain_encoder = True   # set False to skip\n",
    "feature_dim = 256         # encoder output size\n",
    "Nhid = 300                # hidden spiking neurons\n",
    "Nout = 2                 # CIFAR-10 classes\n",
    "T = 25                    # timesteps for spikes\n",
    "\n",
    "# Spiking neuron parameters\n",
    "lam = 0.9              # leak (membrane decay)\n",
    "theta_h_base = 0.5     # hidden threshold base\n",
    "theta_o = 0.5          # output threshold\n",
    "eta_out = 5e-4         # LR for W2 (supervised Hebbian)\n",
    "eta_in = 2e-4          # LR for W1 (κ-based SADP)\n",
    "decay = 0.9995         # mild decay (multiplicative) per update\n",
    "norm_eps = 1e-6\n",
    "clip_w2 = 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03fbdb1-697f-4a64-9de5-7c56763e95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_dataset(root='Data', img_size=(28, 28), batch_size=64):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses custom Lung Image dataset (3 classes).\n",
    "    Automatically splits into training and testing sets (80/20 split).\n",
    "    Each subfolder inside `root` should correspond to one class.\n",
    "    \"\"\"\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "    def load_split(split):\n",
    "        gen = datagen.flow_from_directory(\n",
    "            root,\n",
    "            target_size=img_size,\n",
    "            color_mode='rgb',     # assuming colored images\n",
    "            batch_size=batch_size,\n",
    "            class_mode='sparse',\n",
    "            subset=split,          # 'training' or 'validation'\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        num_samples = gen.samples\n",
    "        x = np.zeros((num_samples, *img_size, 3), dtype=np.float32)\n",
    "        y = np.zeros((num_samples,), dtype=np.int32)\n",
    "\n",
    "        idx = 0\n",
    "        for bx, by in gen:\n",
    "            bsize = bx.shape[0]\n",
    "            x[idx:idx+bsize] = bx\n",
    "            y[idx:idx+bsize] = by\n",
    "            idx += bsize\n",
    "            if idx >= num_samples:\n",
    "                break\n",
    "        return x, y\n",
    "\n",
    "    # ---- Load train and test sets ----\n",
    "    (x_train, y_train) = load_split('training')\n",
    "    (x_test, y_test) = load_split('validation')\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    print(f\"✅ Loaded LUNG_IMAGE_SETS dataset successfully\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n",
    "    print(f\"Classes found: {len(np.unique(y_train))}\")\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4175044c-1d08-4ffb-85cc-cc34a62c216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Poisson Encoder\n",
    "# ---------------------------\n",
    "def poisson_encode_features(batch_feats, T):\n",
    "    B, Nin_local = batch_feats.shape\n",
    "    rnd = np.random.rand(B, T, Nin_local).astype(np.float32)\n",
    "    spikes = (rnd < batch_feats[:, None, :]).astype(np.float32)\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa84d2b-43cc-4c15-a0d7-dc0dfaa7cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "def build_encoder(input_shape=(28, 28, 3), output_dim=256, dropout_rate=0.4, l2_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Robust CNN encoder optimized for small 28x28 color retinal images.\n",
    "    Outputs a feature vector of size `output_dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Block 1 (Keep spatial info early) ---\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # --- Block 2 ---\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # --- Block 3 (compact high-level features) ---\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # --- Dense Projection Head ---\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(output_dim, activation='relu')(x)\n",
    "\n",
    "    model = models.Model(inp, out, name='encoder_small_retina')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2b2cc4-4570-4b47-b6f0-45a6a480b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LIF forward pass (1SADP / 2SADP)\n",
    "def forward_lif_features(x_batch_feats, y_targets=None, teacher_force=False,\n",
    "                         architecture='1SADP', T_local=25):\n",
    "    global W1, W1_2, W2, theta_h, theta_h2, Nin, Nout, Nhid_global, lam, theta_o\n",
    "\n",
    "    B = x_batch_feats.shape[0]\n",
    "\n",
    "    # --------------------------\n",
    "    # Membrane potentials\n",
    "    # --------------------------\n",
    "    Vh  = np.zeros((B, Nhid_global), dtype=np.float32)\n",
    "    Vh2 = np.zeros((B, Nhid_global), dtype=np.float32) if architecture == '2SADP' else None\n",
    "    Vo  = np.zeros((B, Nout), dtype=np.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Spike storage\n",
    "    # --------------------------\n",
    "    spikes_h  = np.zeros((B, T_local, Nhid_global), dtype=np.float32)\n",
    "    spikes_h2 = np.zeros((B, T_local, Nhid_global), dtype=np.float32) if architecture=='2SADP' else None\n",
    "    spikes_o  = np.zeros((B, T_local, Nout), dtype=np.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Input Poisson spikes\n",
    "    # --------------------------\n",
    "    S_in = poisson_encode_features(x_batch_feats, T_local)\n",
    "\n",
    "    # --------------------------\n",
    "    # Time loop\n",
    "    # --------------------------\n",
    "    for t in range(T_local):\n",
    "\n",
    "        # ===== Hidden Layer 1 =====\n",
    "        I_h = np.einsum('bi,ij->bj', S_in[:, t], W1)\n",
    "        Vh = lam * Vh + I_h\n",
    "\n",
    "        spk_h = (Vh > theta_h).astype(np.float32)\n",
    "        Vh[spk_h == 1] = 0.0\n",
    "        spikes_h[:, t, :] = spk_h\n",
    "\n",
    "        # ===== Hidden Layer 2 (2SADP) =====\n",
    "        if architecture == '2SADP':\n",
    "            I_h2 = np.einsum('bi,ij->bj', spk_h, W1_2)\n",
    "            Vh2 = lam * Vh2 + I_h2\n",
    "\n",
    "            spk_h2 = (Vh2 > theta_h2).astype(np.float32)\n",
    "            Vh2[spk_h2 == 1] = 0.0\n",
    "            spikes_h2[:, t, :] = spk_h2\n",
    "\n",
    "            spk_to_output = spk_h2\n",
    "        else:\n",
    "            spk_to_output = spk_h\n",
    "\n",
    "        # ===== Output Layer =====\n",
    "        I_o = np.einsum('bi,ij->bj', spk_to_output, W2)\n",
    "        Vo = lam * Vo + I_o\n",
    "\n",
    "        spk_o = (Vo > theta_o).astype(np.float32)\n",
    "        Vo[spk_o == 1] = 0.0\n",
    "        spikes_o[:, t, :] = spk_o\n",
    "\n",
    "    # Return spikes\n",
    "    return (spikes_h, spikes_h2, spikes_o) if architecture=='2SADP' else (spikes_h, spikes_o)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Weight update per batch\n",
    "# ---------------------------\n",
    "# def update_weights_batch(x_batch_feats, y_batch, spikes_h, spikes_o, architecture='1SADP'):\n",
    "#     global W1, W1_2, W2\n",
    "\n",
    "#     B = x_batch_feats.shape[0]\n",
    "#     targets = np.zeros((B, Nout), dtype=np.float32)\n",
    "#     targets[np.arange(B), y_batch] = 1.0\n",
    "\n",
    "#     # output spike counts\n",
    "#     out_counts = spikes_o.sum(axis=1)\n",
    "#     preds = np.argmax(out_counts, axis=1)\n",
    "\n",
    "#     # Errors\n",
    "#     errors = targets[:, None, :] - spikes_o\n",
    "#     dW2 = np.einsum('bti,btj->ij', spikes_h if architecture=='1SADP' else spikes_h, errors) / float(B)\n",
    "#     W2 += eta_out * dW2\n",
    "\n",
    "#     # Kappa-based SADP for W1\n",
    "#     batch_idx = np.arange(B)[:, None]\n",
    "#     time_idx = np.arange(spikes_o.shape[1])[None, :]\n",
    "#     class_idx = y_batch[:, None]\n",
    "#     target_spikes = spikes_o[batch_idx, time_idx, class_idx].astype(np.float32)\n",
    "\n",
    "#     agree = np.mean(spikes_h == target_spikes[..., None], axis=1).astype(np.float32)\n",
    "#     pa = np.mean(spikes_h, axis=1).astype(np.float32)\n",
    "#     pb = np.mean(target_spikes, axis=1).astype(np.float32)[:, None]\n",
    "#     pe = pa * pb + (1 - pa) * (1 - pb)\n",
    "#     kappa_vals = (agree - pe) / (1.0 - pe + 1e-9)\n",
    "\n",
    "#     dW1 = np.einsum('bi,bj->ij', x_batch_feats, kappa_vals) / float(B)\n",
    "#     W1 += eta_in * dW1\n",
    "\n",
    "#     # decay & normalize\n",
    "#     W1 *= decay\n",
    "#     W2 *= decay\n",
    "#     W1 /= (np.linalg.norm(W1, axis=0, keepdims=True) + norm_eps)\n",
    "#     np.clip(W2, -clip_w2, clip_w2, out=W2)\n",
    "\n",
    "#     return preds, float(np.mean(kappa_vals))\n",
    "\n",
    "\n",
    "\n",
    "def update_weights_batch(x_batch_feats, y_batch,\n",
    "                         spikes_h, spikes_h2, spikes_o,\n",
    "                         architecture='1SADP'):\n",
    "    global W1, W1_2, W2\n",
    "\n",
    "    B, T, _ = spikes_o.shape\n",
    "\n",
    "    # ---------------------------\n",
    "    # One-hot targets\n",
    "    # ---------------------------\n",
    "    targets = np.zeros((B, Nout), dtype=np.float32)\n",
    "    targets[np.arange(B), y_batch] = 1.0\n",
    "\n",
    "    # ---------------------------\n",
    "    # Prediction\n",
    "    # ---------------------------\n",
    "    out_counts = spikes_o.sum(axis=1)\n",
    "    preds = np.argmax(out_counts, axis=1)\n",
    "\n",
    "    # ======================================================\n",
    "    # OUTPUT LAYER UPDATE (supervised Hebbian)\n",
    "    # ======================================================\n",
    "    errors = targets[:, None, :] - spikes_o\n",
    "    pre_out = spikes_h2 if architecture == '2SADP' else spikes_h\n",
    "\n",
    "    dW2 = np.einsum('bti,btj->ij', pre_out, errors) / B\n",
    "    W2 += eta_out * dW2\n",
    "\n",
    "    # ======================================================\n",
    "    # TARGET SPIKES (correct-class output neuron) \n",
    "    # ======================================================\n",
    "    target_spikes = spikes_o[np.arange(B), :, y_batch].astype(np.float32)\n",
    "\n",
    "    # ======================================================\n",
    "    # W1 SADP UPDATE (Input → Hidden-1)\n",
    "    # ======================================================\n",
    "    agree1 = np.mean(spikes_h == target_spikes[:, :, None], axis=1)\n",
    "\n",
    "    pa1 = np.mean(spikes_h, axis=1)\n",
    "    pb  = np.mean(target_spikes, axis=1)[:, None]\n",
    "\n",
    "    pe1 = pa1 * pb + (1.0 - pa1) * (1.0 - pb)\n",
    "    kappa1 = (agree1 - pe1) / (1.0 - pe1 + 1e-9)\n",
    "\n",
    "    dW1 = np.einsum('bi,bj->ij', x_batch_feats, kappa1) / B\n",
    "    W1 += eta_in * dW1\n",
    "\n",
    "    # ======================================================\n",
    "    # W1_2 SADP UPDATE (Hidden-1 → Hidden-2)\n",
    "    # ======================================================\n",
    "    if architecture == '2SADP':\n",
    "        agree2 = np.mean(spikes_h2 == target_spikes[:, :, None], axis=1)\n",
    "        pa2 = np.mean(spikes_h2, axis=1)\n",
    "\n",
    "        pe2 = pa2 * pb + (1.0 - pa2) * (1.0 - pb)\n",
    "        kappa2 = (agree2 - pe2) / (1.0 - pe2 + 1e-9)\n",
    "\n",
    "        dW1_2 = np.einsum(\n",
    "            'bi,bj->ij',\n",
    "            np.mean(spikes_h, axis=1),\n",
    "            kappa2\n",
    "        ) / B\n",
    "\n",
    "        W1_2 += eta_in * dW1_2\n",
    "        W1_2 *= decay\n",
    "        W1_2 /= (np.linalg.norm(W1_2, axis=0, keepdims=True) + norm_eps)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Decay & normalize\n",
    "    # ---------------------------\n",
    "    W1 *= decay\n",
    "    W2 *= decay\n",
    "\n",
    "    W1 /= (np.linalg.norm(W1, axis=0, keepdims=True) + norm_eps)\n",
    "    np.clip(W2, -clip_w2, clip_w2, out=W2)\n",
    "\n",
    "    return preds, float(np.mean(kappa1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a50730-11ab-489a-9553-d8b9b650e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(n_epochs=25, batch_size=128, n_samples=None):\n",
    "    global W1, W2   # ensure weight updates are applied\n",
    "    \n",
    "    n_train = len(train_feats_norm) if n_samples is None else min(n_samples, len(train_feats_norm))\n",
    "    n_epochs_run = 0\n",
    "    epoch_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        n_epochs_run += 1\n",
    "        t0 = time.time()\n",
    "\n",
    "        idx = np.random.permutation(len(train_feats_norm))[:n_train]\n",
    "        num_batches = n_train // batch_size\n",
    "\n",
    "        correct = 0\n",
    "        kappa_log = []\n",
    "\n",
    "        for bi in trange(num_batches, desc=f\"SNN Epoch {epoch+1}/{n_epochs}\"):\n",
    "            s = bi * batch_size\n",
    "            e = s + batch_size\n",
    "            batch_idx = idx[s:e]\n",
    "\n",
    "            Xb = train_feats_norm[batch_idx]\n",
    "            yb = y_train[batch_idx]\n",
    "\n",
    "            # forward\n",
    "            sh, so = forward_lif_features(Xb, teacher_force=False)\n",
    "\n",
    "           \n",
    "            preds, kappa_val = update_weights_batch(\n",
    "                Xb, yb,\n",
    "                spikes_h=sh,\n",
    "                spikes_h2=None,\n",
    "                spikes_o=so,\n",
    "                architecture='1SADP'\n",
    "            )\n",
    "\n",
    "            correct += np.sum(preds == yb)\n",
    "            kappa_log.append(kappa_val)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        acc = correct / float(n_train)\n",
    "        avg_k = float(np.mean(kappa_log)) if kappa_log else 0.0\n",
    "        epoch_accuracies.append(acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{n_epochs} | \"\n",
    "            f\"Train acc = {acc:.4f} | \"\n",
    "            f\"avg κ = {avg_k:.6f} | \"\n",
    "            f\"Time = {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    return None, n_epochs_run, epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce669800-8d89-4d44-b9e5-5f40ed7ecad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Evaluate SNN Function\n",
    "# -----------------------------------------------------\n",
    "def evaluate_snn(n_samples=None, batch_size=256):\n",
    "    n_test = len(test_feats_norm) if n_samples is None else min(n_samples, len(test_feats_norm))\n",
    "    idx = np.random.permutation(len(test_feats_norm))[:n_test]\n",
    "    num_batches = int(np.ceil(n_test / batch_size))\n",
    "    correct = 0\n",
    "\n",
    "    for bi in trange(num_batches, desc=\"SNN Testing\"):\n",
    "        s = bi * batch_size\n",
    "        e = min(s + batch_size, n_test)\n",
    "        batch_idx = idx[s:e]\n",
    "\n",
    "        Xb = test_feats_norm[batch_idx]\n",
    "        yb = y_test[batch_idx]\n",
    "\n",
    "        sh, so = forward_lif_features(Xb, teacher_force=False)\n",
    "        preds = np.argmax(so.sum(axis=1), axis=1)\n",
    "        correct += np.sum(preds == yb)\n",
    "\n",
    "    acc = correct / float(n_test)\n",
    "    print(f\"SNN Test Acc = {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1070495-96b6-44cb-b022-0d05c02eb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name=None, feature_dim=128, encoder_epochs=10,\n",
    "                   batch_size_encoder=128, pretrain_encoder=True,\n",
    "                   Nhid=256, theta_h_base=0.5, seed=42,\n",
    "                   architecture='1SADP', T=25,\n",
    "                   encoding_type='cnn+poisson'):\n",
    "    \"\"\"\n",
    "    encoding_type: 'poisson_only' or 'cnn+poisson'\n",
    "    architecture: '1SADP' or '2SADP'\n",
    "    T: temporal dimension (timesteps)\n",
    "    \"\"\"\n",
    "\n",
    "    global x_train, y_train, x_test, y_test\n",
    "    global train_feats_norm, test_feats_norm\n",
    "    global W1, W1_2, W2, theta_h, theta_h2, Nin, Nout, Nhid_global, lam, theta_o\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Running experiment | {architecture} | T={T} | Encoding: {encoding_type}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Use loaded dataset if available\n",
    "    # ---------------------------\n",
    "    if 'x_train' not in globals() or 'x_test' not in globals():\n",
    "        if dataset_name is None:\n",
    "            raise ValueError(\"No dataset loaded and no dataset_name provided.\")\n",
    "        else:\n",
    "            print(f\"Loading dataset: {dataset_name}\")\n",
    "            (x_train, y_train), (x_test, y_test), input_shape = load_dataset(dataset_name)\n",
    "    else:\n",
    "        print(\"Using already loaded dataset...\")\n",
    "        input_shape = x_train.shape[1:]\n",
    "\n",
    "    # ---------------------------\n",
    "    # Encoder + feature extraction\n",
    "    # ---------------------------\n",
    "    if encoding_type == 'cnn+poisson':\n",
    "        encoder = build_encoder(input_shape=input_shape, output_dim=feature_dim)\n",
    "\n",
    "        if pretrain_encoder:\n",
    "            inp = encoder.input\n",
    "            feat = encoder.output\n",
    "            out = layers.Dense(len(np.unique(y_train)), activation='softmax')(feat)\n",
    "            clf = models.Model(inp, out)\n",
    "            clf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            print(\"Pretraining encoder on provided dataset...\")\n",
    "            clf.fit(x_train, y_train, validation_split=0.1,\n",
    "                    epochs=encoder_epochs, batch_size=batch_size_encoder,\n",
    "                    verbose=2)\n",
    "\n",
    "        print(\"Extracting features from CNN...\")\n",
    "        train_feats = encoder.predict(x_train, batch_size=256, verbose=1)\n",
    "        test_feats  = encoder.predict(x_test, batch_size=256, verbose=1)\n",
    "\n",
    "        minf = train_feats.min(axis=0, keepdims=True)\n",
    "        maxf = train_feats.max(axis=0, keepdims=True)\n",
    "        rangef = (maxf - minf) + 1e-9\n",
    "        train_feats_norm = (train_feats - minf) / rangef\n",
    "        test_feats_norm  = (test_feats  - minf) / rangef\n",
    "\n",
    "    elif encoding_type == 'poisson_only':\n",
    "        train_feats_norm = x_train.reshape(len(x_train), -1).astype(np.float32)\n",
    "        test_feats_norm = x_test.reshape(len(x_test), -1).astype(np.float32)\n",
    "\n",
    "        train_feats_norm /= train_feats_norm.max()\n",
    "        test_feats_norm /= test_feats_norm.max()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"encoding_type must be 'poisson_only' or 'cnn+poisson'\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Initialize network\n",
    "    # ---------------------------\n",
    "    Nin = train_feats_norm.shape[1]\n",
    "    Nout = len(np.unique(y_train))\n",
    "    Nhid_global = Nhid\n",
    "    lam = 0.9\n",
    "    theta_o = 0.5\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.normal(0, 0.1, (Nin, Nhid_global)).astype(np.float32)\n",
    "    W2 = np.random.normal(0, 0.1, (Nhid_global, Nout)).astype(np.float32)\n",
    "    theta_h = (theta_h_base + 0.05 * np.random.randn(Nhid_global)).astype(np.float32)\n",
    "\n",
    "    if architecture == '2SADP':\n",
    "        W1_2 = np.random.normal(0, 0.1, (Nhid_global, Nhid_global)).astype(np.float32)\n",
    "        theta_h2 = (theta_h_base + 0.05 * np.random.randn(Nhid_global)).astype(np.float32)\n",
    "        print(f\"Initialized W1_2: {W1_2.shape}, theta_h2: {theta_h2.shape}\")\n",
    "    else:\n",
    "        W1_2, theta_h2 = None, None\n",
    "\n",
    "    print(f\"Feature dim (Nin): {Nin}, Nhid: {Nhid_global}, Nout: {Nout}\")\n",
    "    if architecture == '1SADP':\n",
    "        print(f\"W1: {W1.shape}, W2: {W2.shape}, θ_h: {theta_h.shape}\")\n",
    "    else:\n",
    "        print(f\"W1: {W1.shape}, W1_2: {W1_2.shape}, W2: {W2.shape}, θ_h: {theta_h.shape}, θ_h2: {theta_h2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1f99a-9fd5-4f33-8d48-a2c470ff8bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
